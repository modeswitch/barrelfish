%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copyright (c) 2011, ETH Zurich.
% All rights reserved.
%
% This file is distributed under the terms in the attached LICENSE file.
% If you do not find this file, copies can be found by writing to:
% ETH Zurich D-INFK, Haldeneggsteig 4, CH-8092 Zurich. Attn: Systems Group.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,twoside]{report} % for a report (default)

\usepackage{bftn,color} % You need this

\title{Barrelfish on the Beehive experimental platform}   % title of report
\author{Richard Black}	% author
\tnnumber{007}  % give the number of the tech report
\tnkey{Beehive} % Short title, will appear in footer

% \date{Month Year} % Not needed - will be taken from version history

%% \newcommand{\note}[1]{}
\newcommand{\note}[1]{[\textcolor{red}{\textit{#1}}]}

\begin{document}
\maketitle

%
% Include version history first
%
\begin{versionhistory}
\vhEntry{1.0}{14.06.2010}{RJB}{Initial version}
%\vhEntry{1.01}{14.06.2010}{AS}{Added discussion of design and unresolved issues}
\end{versionhistory}

% \intro{Abstract}		% Insert abstract here
% \intro{Acknowledgements}	% Uncomment (if needed) for acknowledgements
% \tableofcontents		% Uncomment (if needed) for final draft
% \listoffigures		% Uncomment (if needed) for final draft
% \listoftables			% Uncomment (if needed) for final draft

\chapter{Introduction}

This report describes the port of the Barrlelfish operating system to
the Beehive multi-core experimental platform.

\note{This is what a note does.}

\note{summarise chapters}

\note{executive summary}

\chapter{Barrelfish implementation on Beehive}

\section{Hypervisor}

\section{Memory map}

\section{System calls}

\section{Timer interrupts}

\section{BMP interconnect driver}

\emph{Beehive} is an experimental FPGA-based multicore architecture
under development at Microsoft Research\cite{beehive:hwman} targeting
the RAMP BEE-3 platform\cite{davis:bee3}. The current design has 13
RISC cores on a ring network with a memory controller, gigabit
Ethernet, and other IO devices. Like SCC, each core has a private
non-coherent cache and hardware support for message passing. However,
Beehive's message-passing mechanism bypasses the caches and memory
system entirely.  Each core may directly send a message of up to 63
words to another core on the ring.  Incoming messages are placed in a
FIFO which can be cheaply (three cycles) polled. The FIFO holds 1024
words and has no hardware flow control -- software must ensure that it
does not overflow.

As with SCC, the challenge for a Beehive interconnect driver is to
provide secure efficient multiplexing of the shared hardware message
channel.  We do this at two levels. Between CPU drivers, a single
point-to-point message channel is implemented with credit-based flow
control, with 64 words of the 1024-word buffer reserved for each
sender core.  User channels are multiplexed over this by prepending a
one-word header to each message. At user-level, the interconnect
driver similarly maintains an incoming message buffer. The generated
stubs pack message arguments into variable-length fragments of up to
57 words of payload (the remainder is used for headers or reserved for
credit messages), perform flow-control on the user-level buffer, and
handle incoming messages when notified by the interconnect driver.

\subsection{Kernel BMP protocol}

At the hardware level messages have a single word header containing
the length of the message, the core number and the type followed by
between 1 and 63 words of payload.  Messages of length zero are
reserved for communication between the master core and the debug unit.
A single type (2) is used for BMP messages.  Each core has a 1024 word
FIFO which must not overflow.

To ensure that the FIFO does not overflow the kernel BMP components
use a credit based flow control mechanism.  The first word of the
message (after the hardware header) is divided into two fields; one of
which contains any credit being sent, and the other provides
demultiplexing into multiple user level channels.

Each core is initialised with 64 words of credit for each other core
and once this is consumed transmission must wait for credit to be sent
from the destination.  The sending of a credit acknowledgement also
requires space in the destination FIFO and so itself consumes credit.
To avoid sending a credit acknowledgement for a credit
acknowledgement, credits are not sent if the available amount is less
than or equal to 2 words.

When sending a message the code ensures that it has enough credit to
send the message and still have 2 words of credit remaining.  This
permits it to send an acknowledgement for the other core without first
having to wait for an acknowledgment from the other core, which seems
sufficient to avoid deadlocking (but may not strictly be necessary).

The MTU for BMP messages is calculated as follows.  The hardware
supports 64 words, 62 plus kernel BMP header plus header, but this is
also the flow control limit.  Since we might have sent an ACK already
which is not acked a common value of credit is 62.  Since we have to
keep enough credit to be able to send an ACK subsequent to sending a
message, the maximum sized message is 60 words, i.e. 58 words of
payload.

As a possible future change, we observe that core 0 is always the DRAM
controller, and cores do not need to send themselves messages,
therefore there are a maximum of 14 cores on the ring to divide out
the FIFO between.  This would permit 73 words of credit per core
rather than 64 which would translate into a kernel MTU of 67 instead
of 58 words.

\subsection{Kernel BMP processing}

The state machine for processing BMP has to take carefully into
consideration the fact that there are no interrupts for the FIFO, or
for timers, the multiplexed nature of the FIFO, and the neccessity to
receive credit in order to trasnmit.

One entry point is when the core is idle; the FIFO should be processed
completely until it is idle with any incoming message potentially
causing a domain to become runnable and the scheduler then being
called.  Once the FIFO is idle it should continue to be polled until
the timer interrupt fire.  During this time credit acknowledgements
must be sent as appropriate.

The second entry point is when a send is desired.  This has to deal
with the possibility that there is insufficient credit.  If there is
insufficient credit then the code must process messages, delivering
them on incoming channels, and sending and receiving credit, until
there is enough credit to send the required message.  Then the message
can be sent.  After the message is sent the FIFO must again be
processed until it is idle.

Note that if a message is desired to be sent when there is
insufficient credit the code currently processes the FIO until there
is credit which can be a substantially long time.  An alternative
would be to return a ``busy'' code the caller which would have the
difficulty that it is impossible to subsequently advise when a retry
might succeed.

To mitigate busy waiting on transmit the third entry point is called
on a timer tick before the scheduler is called.  This once again
processes the FIFO until it is idle (including the sending of
acknowledgements).  This processing has the benefit of providing the
scheduler with as much information as possible before the timer tick
is delivered.

There is an additional entry point that currently for debug purposes
the \texttt{sys\_nop} system call also pumps messages from the FIFO.

The \texttt{sys\_yield} call currently does not pump the FIFO, but
probably should.

\subsection{User-level BMP component}

\chapter{Performance measurements}

Several unconventional features of Beehive make performance of any
communication system a challenge.  In particular, the small,
direct-mapped instruction cache and low instruction density mean 
I-cache miss rate tends to dominate performance.
We found that a single word echo between user programs on
different cores has a latency of 56k$\pm$1k cycles with more than 80\%
of this cost attributable to I-cache misses (each taking
$\approx54$ cycles).

It is more instructive to look at the count of instructions executed,
even though instruction density is low.  It takes about 2900
instructions to send a request: 230 in user stubs,
630 in the capability invocation, 100 in sending the message, 600
returning from the system call, and 1300 for stubs, threads package
and kernel to idle. 

The receive path is about 3200 instructions: 100
to receive the message in the CPU driver, 570 to look up the endpoint
capability and copy the message to the user's channel buffer, 480 to
make the process runnable, and a further 2000 to upcall the process
and dispatch the message through the stub to application code.

While the performance of the processor is low, and both Barrelfish
and the FPGA processor core have scope for optimization,
the interconnect driver does provide secure multiplexed
messaging on this non-cache-coherent hardware at a cost which is low
considering the general cost of execution on the platform.



\chapter{General observations}

\bibliographystyle{abbrvnat}
\bibliography{defs,barrelfish}

\end{document}
